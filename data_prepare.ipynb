{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behaviors length: 22034\n",
      "News length: 26740\n",
      "Valid behaviors length: 7538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data prepare...\n",
      "22034it [00:12, 1818.43it/s]\n",
      "INFO:root:Create id2index mapping...\n",
      "INFO:root:Create rating dataframe...\n",
      "INFO:root:Read rating dataframe from data/train/rating.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User num: 5000\n",
      "Item num: 26740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fuminghao/opt/anaconda3/envs/msnews/lib/python3.7/site-packages/ipykernel_launcher.py:610: DtypeWarning: Columns (0,1,2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "INFO:root:Create news-wikiVector dictionary dataframe...\n",
      "INFO:root:Create news-embedding dictionary...\n",
      "INFO:root:Create user affinity matrix...\n",
      "INFO:root:Create items coocurrance matrix...\n",
      "INFO:root:Create item similarity matrix...\n",
      "INFO:root:Using jaccard based similarity to build\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from scipy import sparse\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import pyspark\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from recommenders.utils.python_utils import (\n",
    "    get_top_k_scored_items,\n",
    "    rescale,\n",
    ")\n",
    "\n",
    "from recommenders.utils.spark_utils import start_or_get_spark\n",
    "from recommenders.evaluation.spark_evaluation import SparkRankingEvaluation, SparkRatingEvaluation\n",
    "from recommenders.evaluation.python_evaluation import auc, logloss\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "\n",
    "COOCCUR = \"cooccurrence\"\n",
    "JACCARD = \"jaccard\"\n",
    "LIFT = \"lift\"\n",
    "\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "\n",
    "\n",
    "\n",
    "MIND_type = 'demo'\n",
    "data_path = 'data'  # temp dir\n",
    "\n",
    "class MINDdata:\n",
    "    def __init__(self, MIND_type='demo', data_path='data'):\n",
    "        self.train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "        self.train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "        self.valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
    "        self.valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
    "        self.entity_embedding_file = os.path.join(data_path, 'valid', r'entity_embedding.vec')\n",
    "        self.relation_embedding_file = os.path.join(data_path, 'valid', r'relation_embedding.vec')\n",
    "\n",
    "        mind_url, mind_train_dataset, mind_dev_dataset, mind_utils = get_mind_data_set(MIND_type)\n",
    "\n",
    "        if not os.path.exists(self.train_news_file):\n",
    "            download_deeprec_resources(mind_url, os.path.join(data_path, 'train'), mind_train_dataset)\n",
    "\n",
    "        if not os.path.exists(self.valid_news_file):\n",
    "            download_deeprec_resources(mind_url, \\\n",
    "                                       os.path.join(data_path, 'valid'), mind_dev_dataset)\n",
    "\n",
    "        # Train dataset\n",
    "        self.behaviors = pd.read_csv(self.train_behaviors_file, sep='\\t', names=['Impression ID',\n",
    "                                                            'User ID',\n",
    "                                                            'Time',\n",
    "                                                            'History',\n",
    "                                                            'Impressions'])\n",
    "\n",
    "        self.news = pd.read_csv(self.train_news_file, sep='\\t', names=['News ID',\n",
    "                                                            'Category',\n",
    "                                                            'SubCategory',\n",
    "                                                            'Title',\n",
    "                                                            'Abstract',\n",
    "                                                            'URL',\n",
    "                                                            'Title Entities',\n",
    "                                                            'Abstract Entities'])\n",
    "        print(f'Behaviors length: {len(self.behaviors)}')\n",
    "        print(f'News length: {len(self.news)}')\n",
    "\n",
    "        # Valid dataset\n",
    "        self.behaviors_dev = pd.read_csv(self.valid_behaviors_file, sep='\\t', names=['Impression ID',\n",
    "                                                            'User ID',\n",
    "                                                            'Time',\n",
    "                                                            'History',\n",
    "                                                            'Impressions'])\n",
    "\n",
    "        print(f'Valid behaviors length: {len(self.behaviors_dev)}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MINDmodel:\n",
    "    def __init__(\n",
    "            self,\n",
    "            data,\n",
    "            col_user='User ID',\n",
    "            col_item='News ID',\n",
    "            col_time='Time',\n",
    "            col_rating='Rating',\n",
    "            col_history='History',\n",
    "            col_impression='Impression ID',\n",
    "            col_prediction='Prediction',\n",
    "            time_decay_flag=True,\n",
    "            similarity_type=JACCARD,\n",
    "            threshold=1,\n",
    "            time_now=None\n",
    "    ):\n",
    "        self.col_user = col_user\n",
    "        self.col_item = col_item\n",
    "        self.col_time = col_time\n",
    "        self.col_rating = col_rating\n",
    "        self.col_history = col_history\n",
    "        self.col_impression = col_impression\n",
    "        self.col_timedecay = None\n",
    "        self.similarity_type = similarity_type\n",
    "        self.threshold=threshold\n",
    "        self.time_now = time_now\n",
    "        self.time_decay_flag = time_decay_flag\n",
    "        self.col_prediction = col_prediction\n",
    "\n",
    "        self.n_users = None\n",
    "        self.n_items = None\n",
    "\n",
    "        self.user2index = None\n",
    "        self.item2index = None\n",
    "\n",
    "        self.index2item = None\n",
    "        self.index2user = None\n",
    "\n",
    "        self.rating_df = None\n",
    "        self.user_item_time_dict = None\n",
    "        self.click_df = None\n",
    "        self.wiki_dict = None\n",
    "\n",
    "        self.affinity_matrix = None\n",
    "\n",
    "        self.user_coocurrence = None\n",
    "        self.u2u_sim = None\n",
    "\n",
    "        self.item_coocurrence = None\n",
    "        self.i2i_sim = None\n",
    "\n",
    "        self.item_embedding_dict = None\n",
    "\n",
    "\n",
    "    def get_recent_behaviors_dict(self):\n",
    "        user_hist_imp = {}\n",
    "        for i, row in tqdm(data.behaviors.iterrows()):\n",
    "            if user_hist_imp.get(row['User ID']) is None:\n",
    "                user_hist_imp[row['User ID']] = str(row['History']).split(' '), str(row['Impressions']).split(' '), pd.to_datetime(row['Time'])\n",
    "            else:\n",
    "                if pd.to_datetime(row['Time']) > user_hist_imp[row['User ID']][2]:\n",
    "                    user_hist_imp[row['User ID']] = str(row['History']).split(' '), user_hist_imp[row['User ID']][1] + str(row['Impressions']).split(' '), pd.to_datetime(row['Time'])\n",
    "                else:\n",
    "                    user_hist_imp[row['User ID']] = user_hist_imp[row['User ID']][0], user_hist_imp[row['User ID']][1] + str(row['Impressions']).split(' '), user_hist_imp[row['User ID']][2]\n",
    "\n",
    "        data.behaviors = user_hist_imp\n",
    "\n",
    "\n",
    "    def set_index(self, behaviors_dict):\n",
    "        '''\n",
    "        Mapping user and item id to index\n",
    "        :param behaviors_df: User impression history dataframe\n",
    "        :param news_df: news dataframe\n",
    "        :return: None\n",
    "        '''\n",
    "        logger.info('Create id2index mapping...')\n",
    "\n",
    "        self.index2user = dict(enumerate(behaviors_dict.keys()))\n",
    "        self.user2index = {v: k for k, v in self.index2user.items()}\n",
    "\n",
    "        self.index2item = dict(enumerate(data.news[self.col_item]))\n",
    "        self.item2index = {v: k for k, v in self.index2item.items()}\n",
    "\n",
    "        self.n_users = len(self.index2user)\n",
    "        self.n_items = len(self.index2item)\n",
    "\n",
    "        print('User num: {}'.format(self.n_users))\n",
    "        print('Item num: {}'.format(self.n_items))\n",
    "\n",
    "\n",
    "    def get_rating_df(self, behaviors):\n",
    "        '''\n",
    "        Get all click history with rating and timedecay\n",
    "        :param behaviors: User impressions history dataframe\n",
    "        :return: None\n",
    "        '''\n",
    "        def set_time_decay(df, col_timedecay, half_life):\n",
    "            '''\n",
    "            Calculate click actions timedecay\n",
    "            :param df: behaviors dataframe\n",
    "            :param col_timed: name of time column in df\n",
    "            :param half_life: hyper parameter T\n",
    "            :return: None\n",
    "            '''\n",
    "            logger.info('get time decay of rating dataframe...')\n",
    "\n",
    "            self.col_timedecay = col_timedecay\n",
    "            if self.time_now is None:\n",
    "                self.time_now = pd.to_datetime(df[self.col_time]).max()\n",
    "            print(f'time now: {self.time_now}')\n",
    "\n",
    "            df[self.col_timedecay] = df.apply(\n",
    "                lambda x: pd.to_numeric(x[self.col_rating]) * np.power(0.5, (\n",
    "                            pd.to_datetime(self.time_now) - pd.to_datetime(x[self.col_time])).days / half_life),\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        logger.info('Create rating dataframe...')\n",
    "\n",
    "        rating_df_path = os.path.join('data','train','rating.csv')\n",
    "        if not os.path.exists(rating_df_path):\n",
    "            lst = []\n",
    "            for k, v in tqdm(behaviors.items()):\n",
    "                for hist in v[0]:\n",
    "                    if self.item2index.get(hist) is not None:\n",
    "                        lst.append([self.user2index[k], self.item2index[hist], 2])\n",
    "                for imp in v[1]:\n",
    "                    if imp[-1] == '1':\n",
    "                        lst.append([self.user2index[k], self.item2index[imp[:-2]], 3])\n",
    "                    else:\n",
    "                        lst.append([self.user2index[k], self.item2index[imp[:-2]], 1])\n",
    "\n",
    "            self.rating_df = pd.DataFrame(lst, columns=[self.col_user, self.col_item, self.col_rating])\n",
    "            # timedecay\n",
    "            self.rating_df.to_csv(rating_df_path,index=False)\n",
    "\n",
    "            logger.info(f'Rating dataframe has been saved as {rating_df_path}')\n",
    "\n",
    "        self.col_timedecay = 'Timedecay'\n",
    "\n",
    "        logger.info(f'Read rating dataframe from {rating_df_path}')\n",
    "        self.rating_df = pd.read_csv(rating_df_path,names=\n",
    "            [self.col_user, self.col_item, self.col_rating]).iloc[1:]\n",
    "        self.rating_df = self.rating_df.astype(\n",
    "            {self.col_user: 'int64', self.col_item: 'int64', self.col_rating: 'float64'})\n",
    "#         if self.time_decay_flag is True:\n",
    "#             self.rating_df[self.col_rating] *= self.rating_df[self.col_timedecay]\n",
    "\n",
    "\n",
    "\n",
    "    def get_click_df(self, behaviors):\n",
    "        '''\n",
    "        Create id to number dictionary\n",
    "        :param behaviors: dataframe\n",
    "        :return: None\n",
    "        '''\n",
    "        logger.info('Create click dataframe...')\n",
    "\n",
    "        click_df_path = os.path.join('data', 'train', 'click.csv')\n",
    "        if not os.path.exists(click_df_path):\n",
    "            lst = []\n",
    "            for i, row in tqdm(behaviors.iterrows()):\n",
    "                imp_lst = row['Impressions'].split(' ')\n",
    "                for imp in imp_lst:\n",
    "                    if imp[-1] == '1':\n",
    "                        lst.append([self.user2index[row['User ID']], self.item2index[imp[:-2]], row[self.col_time]])\n",
    "\n",
    "            self.click_df = pd.DataFrame(lst, columns=[self.col_user, self.col_item, self.col_time])\n",
    "            self.click_df.to_csv(click_df_path,index=False)\n",
    "            logger.info(f'Click dataframe has been saved as {click_df_path}')\n",
    "\n",
    "        logger.info(f'Read click dataframe from {click_df_path}')\n",
    "        self.click_df = pd.read_csv(click_df_path, names=[self.col_user, self.col_item, self.col_time]).iloc[1:]\n",
    "\n",
    "    def get_item_topk_click(self, click_df, k):\n",
    "        return click_df[self.col_item].value_counts().index[:k]\n",
    "\n",
    "    def get_hist_and_last_clicks(self, click_df):\n",
    "        '''\n",
    "        Get user the last click and other click history\n",
    "        :param click_df: click history\n",
    "        :return:\n",
    "        click_hist_df: all users click history except the last one\n",
    "        click_last_df: all users last click\n",
    "        '''\n",
    "        click_df = click_df.sort_values(by=[self.col_user, self.col_time])\n",
    "        click_last_df = click_df.groupby(self.col_user).tail(1)\n",
    "\n",
    "        def hist_func(user_df):\n",
    "            if len(user_df) == 1:\n",
    "                return user_df\n",
    "            else:\n",
    "                return user_df[:-1]\n",
    "\n",
    "        click_hist_df = click_df.groupby('User ID').apply(hist_func).reset_index(drop=True)\n",
    "\n",
    "        return click_hist_df, click_last_df\n",
    "\n",
    "    def get_user_item_time_dict(self, click_df):\n",
    "        '''\n",
    "        Create user-[(item1, time1), (item2, time2)...] dictionary\n",
    "        :param click_df: click history dataframe\n",
    "        :return: None\n",
    "        '''\n",
    "        logger.info('Create user-(item, time) dictionary dataframe...')\n",
    "\n",
    "        click_df = click_df.sort_values('Time')\n",
    "\n",
    "        def make_item_time_pair(df):\n",
    "            return list(zip(df['News ID'], df['Time']))\n",
    "\n",
    "        user_item_time_df = click_df.groupby('User ID')['News ID', 'Time'] \\\n",
    "            .apply(lambda x: make_item_time_pair(x)).reset_index().rename(columns={0: 'News-Time list'})\n",
    "\n",
    "        self.user_item_time_dict = dict(zip(user_item_time_df['User ID'], user_item_time_df['News-Time list']))\n",
    "\n",
    "\n",
    "    def get_wiki_dict(self, en_path, re_path):\n",
    "        '''\n",
    "        Create WikidataId-embedding_vector dictionary\n",
    "        :param en_path: entity_embedding file\n",
    "        :param re_path: relation_embedding file\n",
    "        :return: None\n",
    "        '''\n",
    "        logger.info('Create news-wikiVector dictionary dataframe...')\n",
    "\n",
    "        res = []\n",
    "        with open(en_path) as f:\n",
    "            for line in f:\n",
    "                lst = line.split('\\t')\n",
    "                res.append([lst[0], lst[1:101]])\n",
    "\n",
    "        with open(re_path) as f:\n",
    "            for line in f:\n",
    "                lst = line.split('\\t')\n",
    "                res.append([lst[0], lst[1:101]])\n",
    "\n",
    "        tmp = pd.DataFrame(res)\n",
    "        self.wiki_dict = dict(zip(tmp[0], tmp[1]))\n",
    "\n",
    "\n",
    "\n",
    "    def get_item_embedding_dict(self, news, wiki_dict):\n",
    "        '''\n",
    "        Create item_embedingVector dictionary\n",
    "        :param news: all items dataframe\n",
    "        :param wiki_dict: WikidataId-embeddingVector dictionary\n",
    "        :return: None\n",
    "        '''\n",
    "        logger.info('Create news-embedding dictionary...')\n",
    "\n",
    "        def _str_find_all_prefix(str, sub):\n",
    "            lst = []\n",
    "            start = 0\n",
    "            while True:\n",
    "                start = str.find(sub, start)\n",
    "                if start == -1:\n",
    "                    return lst\n",
    "                start += len(sub)\n",
    "                lst.append(start)\n",
    "\n",
    "        d_lst = []\n",
    "\n",
    "        for info in news['Title Entities']:\n",
    "            vec = [0.] * 100\n",
    "            w_lst = []\n",
    "            for a in _str_find_all_prefix(str(info), '\\\"WikidataId\\\": \\\"'):\n",
    "                s = ''\n",
    "                while info[a] != '\"':\n",
    "                    s += info[a]\n",
    "                    a += 1\n",
    "                if wiki_dict.get(s):\n",
    "                    w_lst.append(s)\n",
    "\n",
    "            for s in w_lst:\n",
    "                for i, v in enumerate(wiki_dict[s]):\n",
    "                    vec[i] += float(v)\n",
    "\n",
    "            divisor = 1 if len(w_lst) == 0 else len(w_lst)\n",
    "            d_lst.append([v / divisor for v in vec])\n",
    "\n",
    "        self.item_embedding_dict = dict(zip(news[self.col_item], d_lst))\n",
    "\n",
    "\n",
    "    def get_affinity_matrix(self, df):\n",
    "        '''\n",
    "        Calculate user-item affinity matrix\n",
    "        :param df: rating/click dataframe\n",
    "        :return: None\n",
    "        '''\n",
    "        logger.info('Create user affinity matrix...')\n",
    "\n",
    "        self.affinity_matrix = sparse.coo_matrix(\n",
    "            (pd.to_numeric(df[self.col_rating]), (df[self.col_user], df[self.col_item])),\n",
    "            shape=(self.n_users, self.n_items),\n",
    "        ).tocsr()\n",
    "\n",
    "\n",
    "    def get_items_coocurrence_matrix(self, df):\n",
    "        '''\n",
    "        Calculate coocurence matrix for item-item similarity\n",
    "        :param df: user-item impressions dataframe\n",
    "        :return: None\n",
    "        '''\n",
    "        logger.info('Create items coocurrance matrix...')\n",
    "\n",
    "        user_item_hits_mat = sparse.coo_matrix(\n",
    "            (np.repeat(1, df.shape[0]), (df[self.col_user], df[self.col_item])),\n",
    "            shape=(self.n_users, self.n_items),\n",
    "        ).tocsr()\n",
    "\n",
    "        self.item_cooccurrence = user_item_hits_mat.transpose().dot(user_item_hits_mat)\n",
    "        self.item_cooccurrence = self.item_cooccurrence.multiply(\n",
    "            self.item_cooccurrence >= self.threshold\n",
    "        ).toarray()\n",
    "\n",
    "\n",
    "    def get_item_similarity_matrix(self, type):\n",
    "\n",
    "        def _jaccard(cooccurrence_mat):\n",
    "\n",
    "            diag = cooccurrence_mat.diagonal() # Items in cooccurence matrix must be found in behaviors, otherwise dignoal includes zero\n",
    "            diag_rows = np.expand_dims(diag, axis=0) # For broadcast\n",
    "            diag_cols = np.expand_dims(diag, axis=1)\n",
    "\n",
    "            with np.errstate(invalid=\"ignore\", divide=\"ignore\"): # cij/(cii + cjj - cij)\n",
    "                result = cooccurrence_mat / (diag_rows + diag_cols - cooccurrence_mat)\n",
    "\n",
    "            return np.array(result)\n",
    "\n",
    "        def _lift(cooccurrence_mat):\n",
    "\n",
    "            diag = cooccurrence_mat.diagonal()\n",
    "            diag_rows = np.expand_dims(diag, axis=0)\n",
    "            diag_cols = np.expand_dims(diag, axis=1)\n",
    "\n",
    "            with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "                result = cooccurrence_mat / (diag_rows * diag_cols)\n",
    "\n",
    "            return np.array(result)\n",
    "\n",
    "\n",
    "        self.similarity_type = type\n",
    "        logger.info('Create item similarity matrix...')\n",
    "        if self.similarity_type == COOCCUR:\n",
    "            logger.info('Using co-occurrence based similarity to build')\n",
    "            self.i2i_sim = self.item_cooccurrence\n",
    "        elif self.similarity_type == JACCARD:\n",
    "            logger.info('Using jaccard based similarity to build')\n",
    "            self.i2i_sim = _jaccard(self.item_cooccurrence)\n",
    "        elif self.similarity_type == LIFT:\n",
    "            logger.info('Using lift based similarity to build')\n",
    "            self.i2i_sim = _lift(self.item_cooccurrence)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown similarity type: {self.similarity_type}\")\n",
    "\n",
    "\n",
    "    def get_user_coocurrence_matrix(self, df):\n",
    "        '''\n",
    "        Calculate coocurence matrix for user-user similarity\n",
    "        :param df: user-item impressions dataframe\n",
    "        :return: None\n",
    "        '''\n",
    "        logger.info('Create items coocurrance matrix...')\n",
    "\n",
    "        user_item_hits_mat = sparse.coo_matrix(\n",
    "            (np.repeat(1, df.shape[0]), (df[self.col_user], df[self.col_item])),\n",
    "            shape=(self.n_users, self.n_items),\n",
    "        ).tocsr()\n",
    "\n",
    "        self.user_cooccurrence = user_item_hits_mat.dot(user_item_hits_mat.transpose())\n",
    "        self.user_cooccurrence = self.user_cooccurrence.multiply(\n",
    "            self.user_cooccurrence >= self.threshold\n",
    "        ).toarray()\n",
    "\n",
    "\n",
    "    def get_user_similarity_matrix(self, type):\n",
    "\n",
    "        def _jaccard(cooccurrence_mat):\n",
    "\n",
    "            diag = cooccurrence_mat.diagonal() # Items in cooccurence matrix must be found in behaviors, otherwise dignoal includes zero\n",
    "            diag_rows = np.expand_dims(diag, axis=0) # For broadcast\n",
    "            diag_cols = np.expand_dims(diag, axis=1)\n",
    "\n",
    "            with np.errstate(invalid=\"ignore\", divide=\"ignore\"): # cij/(cii + cjj - cij)\n",
    "                result = cooccurrence_mat / (diag_rows + diag_cols - cooccurrence_mat)\n",
    "\n",
    "            return np.array(result)\n",
    "\n",
    "        def _lift(cooccurrence_mat):\n",
    "\n",
    "            diag = cooccurrence_mat.diagonal()\n",
    "            diag_rows = np.expand_dims(diag, axis=0)\n",
    "            diag_cols = np.expand_dims(diag, axis=1)\n",
    "\n",
    "            with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "                result = cooccurrence_mat / (diag_rows * diag_cols)\n",
    "\n",
    "            return np.array(result)\n",
    "\n",
    "\n",
    "        self.similarity_type = type\n",
    "        logger.info('Create item similarity matrix...')\n",
    "        if self.similarity_type == COOCCUR:\n",
    "            logger.info('Using co-occurrence based similarity to build')\n",
    "            self.u2u_sim = self.item_cooccurrence\n",
    "        elif self.similarity_type == JACCARD:\n",
    "            logger.info('Using jaccard based similarity to build')\n",
    "            self.u2u_sim = _jaccard(self.user_cooccurrence)\n",
    "        elif self.similarity_type == LIFT:\n",
    "            logger.info('Using lift based similarity to build')\n",
    "            self.u2u_sim = _lift(self.user_cooccurrence)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown similarity type: {self.similarity_type}\")\n",
    "\n",
    "    def get_user_activate_degree_dict(self, df):\n",
    "        click_times_df = df.groupby(self.col_user)[self.col_item].count().reset_index()\n",
    "        mm = MinMaxScaler()\n",
    "\n",
    "        # normalization\n",
    "        click_times_df[self.col_item] = mm.fit_transform(click_times_df[[self.col_item]])\n",
    "        user_activate_degree_dict = dict(zip(click_times_df[self.col_user], click_times_df[self.col_item]))\n",
    "\n",
    "        return user_activate_degree_dict\n",
    "\n",
    "    def score_all_items(self, test):\n",
    "        '''\n",
    "        Score all items for test users\n",
    "        :return: None\n",
    "        '''\n",
    "        logger.info('Calculate all items score...')\n",
    "\n",
    "        user_ids = list(\n",
    "            map(\n",
    "                lambda user: self.user2index.get(user, np.NAN),\n",
    "                test[self.col_user].unique()\n",
    "            ) # mapping test user to index\n",
    "        )\n",
    "        if any(np.isnan(user_ids)): # np.isnan return a nparray\n",
    "            raise ValueError('Model cannot score users that are not in train dataset')\n",
    "\n",
    "        test_scores = self.user_affinity[user_ids, :].dot(self.item_similarity)\n",
    "        if isinstance(test_scores, sparse.spmatrix):\n",
    "            test_scores = test_scores.toarray()\n",
    "\n",
    "        return test_scores\n",
    "\n",
    "    def get_pop_topk(self, k, sort_topk: bool):\n",
    "        '''\n",
    "        Get top k popular items, according to item impressing times, which can be directly\n",
    "        got from coocurrence matrix diagonal\n",
    "        :param k: k items got\n",
    "        :param sort_topk: whether sort top k items\n",
    "        :return: item-prediction(aka score) dataframe\n",
    "        '''\n",
    "        logger.info('Calculate top {} popular items...'.format(k))\n",
    "        imp_counts = self.item_cooccurrence.diagonal()\n",
    "\n",
    "        # utils: score[test_user_idx, top_items] ?\n",
    "        def topk_index_value(k, arr):\n",
    "            if (k > len(arr)):\n",
    "                logger.warning('k must be less than the array size')\n",
    "                k = len(arr)\n",
    "\n",
    "            top_items = sorted(range(len(arr)), key=lambda x: arr[x])[-k:]\n",
    "            top_scores = [arr[i] for i in top_items]\n",
    "            return np.array(top_items), np.array(top_scores)\n",
    "\n",
    "        top_items, top_scores = topk_index_value(k, imp_counts)\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                self.col_item: [self.index2item[item] for item in top_items.flatten()],\n",
    "                self.col_prediction: top_scores.flatten(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def get_sim_item_topk(self, items, k, sort_topk: bool):\n",
    "        '''\n",
    "        Get top k items according to similarity matrix\n",
    "        :param items: Input items\n",
    "        :param k: number of items\n",
    "        :param sort_topk: whether sort top k items\n",
    "        :return: top k items\n",
    "        '''\n",
    "        # convert id to indices\n",
    "        print(items)\n",
    "        item_indices = np.asarray(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda x: self.item2index.get(x, np.NaN),\n",
    "                    items\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        print(f'item_indices: {item_indices}')\n",
    "\n",
    "        hist_item_num = len(self.i2i_sim)\n",
    "        gen_sim = np.zeros((1, hist_item_num))\n",
    "        for i in item_indices:\n",
    "            gen_sim += self.i2i_sim[i]\n",
    "\n",
    "        sorted_sim_item = sorted(range(hist_item_num), key=lambda x: gen_sim[x])\n",
    "        return sorted_sim_item[-k:]\n",
    "\n",
    "    def get_dev_userid_hist_dict(self, dev_df):\n",
    "\n",
    "        logger.info('Create dev user-historyClick dictionary...')\n",
    "\n",
    "        dev_userid_hist_dict = {}\n",
    "        for i, row in tqdm(dev_df.iterrows()):\n",
    "            if i == 0 or i == 1:\n",
    "                print(row[self.col_history])\n",
    "            user_click_hist = str(row[self.col_history]).split(' ')\n",
    "            dev_userid_hist_dict[row[self.col_user]] = user_click_hist\n",
    "\n",
    "        return dev_userid_hist_dict\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "\n",
    "        logger.info('Data prepare...')\n",
    "        self.get_recent_behaviors_dict()\n",
    "        self.set_index(data.behaviors)\n",
    "        self.get_rating_df(data.behaviors)\n",
    "        self.get_wiki_dict(data.entity_embedding_file, data.relation_embedding_file)\n",
    "        self.get_item_embedding_dict(data.news, self.wiki_dict)\n",
    "        self.get_affinity_matrix(self.rating_df)\n",
    "        self.get_items_coocurrence_matrix(self.rating_df)\n",
    "        self.get_item_similarity_matrix(JACCARD)\n",
    "\n",
    "\n",
    "    def predict(self, df):\n",
    "\n",
    "        pop_topk = self.get_pop_topk(200, False)\n",
    "        user_hist_dict = self.get_dev_userid_hist_dict(df)\n",
    "        for i, row in tqdm(df.iterrows()):\n",
    "            if self.user2index.get(row[self.col_user]) is not None:\n",
    "                sim_topk = model.get_sim_item_topk(user_hist_dict[row[self.col_user]], 5, False)\n",
    "                print(sim_topk)\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = MINDdata()\n",
    "    model = MINDmodel(data)\n",
    "    model.fit()\n",
    "#     model.predict(data.behaviors_dev)\n",
    "#     spark = start_or_get_spark()\n",
    "#     rating_true = spark.createDataFrame(model.rating_df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 1.        , 0.02702703, ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.02702703, 1.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 1.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 1.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        1.        ]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.i2i_sim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}